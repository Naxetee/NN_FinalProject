{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Gradient Descent Without Backpropagation</h1>\n",
    "<h4 style=\"text-align: center;\">Author: Ignacio &Aacute;vila Reyes</h4>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using backpropagation to compute gradients of functios in order to design machine learning has been the order of the day for a long time.\n",
    "\n",
    "Here we present an alternative method which is called **Forward Gradient** and its main advantage is computing the gradient during the forward step. Roughly speaking, this is an unbiased estimate of the gradient that permits us to entirely remove the backward step during the training of a neural network.\n",
    "\n",
    "Let's explain briefly each one of the methods:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Mode\n",
    "\n",
    "Given a function $f:\\mathbb{R}^n\\longrightarrow\\mathbb{R}^m$ and the values $\\theta\\in\\mathbb{R}^n$, $v\\in\\mathbb{R}^n$. *Forward Mode* computes $f(\\theta)$ and the jacobian vector product $J_f(\\theta)\\cdot v$ where $v$ is a vector of perturbations. All of this computed in just the **Forward Step**.\n",
    "\n",
    "<div align=\"center\"><img alt=\"Forward Step Scheme\" src=\"./src/images/fwdStep.png\"></div>\n",
    "\n",
    "Firstly, let's get in touch with the method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow\n",
    "import functorch as fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Let's try to use CUDA\n",
    "DEVICE = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6185, -1.1474]])\n"
     ]
    }
   ],
   "source": [
    "# We define an input\n",
    "input = torch.tensor([[5., 10.]]).to(DEVICE)\n",
    "\n",
    "# Our random vector\n",
    "v = torch.randn_like(input)\n",
    "print(v)\n",
    "\n",
    "# A function\n",
    "def f(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: tensor([[ 25., 100.]])\n",
      "Gradient: tensor([[  6.1849, -22.9488]])\n"
     ]
    }
   ],
   "source": [
    "# funtorch.jvp(f, input, vector) returns f(input) and the directional gradient of\n",
    "# \"f\" in \"input\" with direction \"vector\"\n",
    "value, grad = fc.jvp(f, (input,), (v.to(DEVICE),))\n",
    "print(\"Results:\", value)\n",
    "print(\"Gradient:\",grad)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Mode\n",
    "\n",
    "Given a function $f:\\mathbb{R}^n\\longrightarrow\\mathbb{R}^m$ and the values $\\theta\\in\\mathbb{R}^n$, $v\\in\\mathbb{R}^n$. *Reverse Mode* computes $f(\\theta)$ and the vector-jacobian product $v^T\\cdot J_f(\\theta)$ where $v$ is a vector of adjoints.\n",
    "\n",
    "<div align=\"center\"><img alt=\"Backward Step Scheme\" src=\"./src/images/bckStep.png\"></div>\n",
    "\n",
    "We have already got in touch with this during lab sessions, but let's make some basic calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[ 1.0963, -1.5237,  0.2426, -0.3973, -1.2430],\n",
      "        [ 0.9085, -0.5564, -2.6606,  0.9741, -0.2563],\n",
      "        [ 0.1526, -0.3247, -0.7983, -0.0370, -2.5105]], requires_grad=True)\n",
      "Target: tensor([[ 0.9612, -1.4112, -0.5299, -0.3602, -1.5632],\n",
      "        [-1.4272,  0.2704,  0.4153,  1.3643,  0.6472],\n",
      "        [-0.6394, -0.3596,  0.0329, -0.7257, -0.4160]])\n",
      "Prediction: tensor([[1.0963, 0.0000, 0.2426, 0.0000, 0.0000],\n",
      "        [0.9085, 0.0000, 0.0000, 0.9741, 0.0000],\n",
      "        [0.1526, 0.0000, 0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "Gradient: tensor([[ 0.0000, -0.2032,  0.0000, -0.0530, -0.1657],\n",
      "        [ 0.0000, -0.0742, -0.3547,  0.0000, -0.0342],\n",
      "        [ 0.0000, -0.0433, -0.1064, -0.0049, -0.3347]])\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "# Inputs and expected predictions\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "\n",
    "print(\"Input:\", input)\n",
    "print(\"Target:\", target)\n",
    "\n",
    "# We define a model with a simple activation layer using the relu\n",
    "# function\n",
    "pred = input.relu()\n",
    "print(\"Prediction:\", pred)\n",
    "out = loss(input , pred)\n",
    "out.backward()\n",
    "\n",
    "# Gradient\n",
    "print(\"Gradient:\", input.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Gradients\n",
    "\n",
    "**Definition.** Given a function $f:\\mathbb{R}^n\\longrightarrow\\mathbb{R}$ we define the \"forward gradient\" $g:\\mathbb{R}^n\\longrightarrow\\mathbb{R}^n$ as $g(\\theta)=(\\nabla f(\\theta)\\cdot v)\\hspace{0.1cm}v)$ where $\\theta\\in\\mathbb{R}^n$ is the point at which we are evaluating the gradient and $v\\in\\mathbb{R}^n$ is a perturbation vector taken as a multivariate random variable $v\\sim p(v)$ such that $v_i$ components has *zero mean* and *unit variance*. So that $\\nabla f(\\theta)\\cdot v$ is the directional derivative of $f$ at point $\\theta$ in direction $v$.\n",
    "\n",
    "So each time we evaluate the **forward gradient**, we simply do the following:\n",
    "- Sample random perturbation vector $v\\sim p(v)$\n",
    "- Evaluate $f(\\theta)$ and $\\nabla f(\\theta)\\cdot v$ simoultaneously in the same single forward step without having to compute $\\nabla f$ at all in the process.\n",
    "- Multiply the scalar directional derivative $\\nabla f(\\theta)\\cdot v$ and obtain $g(\\theta)$, the forward gradient.\n",
    "\n",
    "<div align=\"center\"><img alt=\"Forward Gradient Graph\" src=\"./src/images/graph.png\"></div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using \", torch.cuda.get_device_name())\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    \n",
    "!python .\\src\\global_optimization_backprop.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using \", torch.cuda.get_device_name())\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    \n",
    "!python .\\src\\global_optimization_fwdgrad.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "818c43366d05262e85c3ded0ab6de574a0ebd60a97c552125bc88586dca7326c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
